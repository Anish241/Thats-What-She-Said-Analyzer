# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11aZ7peqQ_uriBHw43QXIao7361vdSC8g
"""

import numpy as np
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional
from keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint

MAX_NB_WORDS = 50000  
MAX_SEQUENCE_LENGTH = 100  
VALIDATION_SPLIT = 0.2  
EMBEDDING_DIM = 100  
LSTM_UNITS = 64  
BATCH_SIZE = 32  
EPOCHS = 10  

with open('twss_y.txt', 'r') as f:
    applicable_sentences = f.readlines()

with open('twss_n.txt', 'r') as f:
    not_applicable_sentences = f.readlines()

tokenizer = Tokenizer(num_words=MAX_NB_WORDS)
tokenizer.fit_on_texts(applicable_sentences + not_applicable_sentences)

sequences_applicable = tokenizer.texts_to_sequences(applicable_sentences)
sequences_not_applicable = tokenizer.texts_to_sequences(not_applicable_sentences)

word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))

data = pad_sequences(sequences_applicable + sequences_not_applicable, maxlen=MAX_SEQUENCE_LENGTH)
labels = np.concatenate([np.ones(len(sequences_applicable)), np.zeros(len(sequences_not_applicable))])
labels = to_categorical(labels)
print('Shape of data tensor:', data.shape)
print('Shape of label tensor:', labels.shape)

indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]

model = Sequential()
model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
model.add(Bidirectional(LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2)))
model.add(Dense(2, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

checkpoint = ModelCheckpoint('twss_model.h5', monitor='val_loss', save_best_only=True, verbose=1)
callbacks = [checkpoint]

history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_val, y_val), callbacks=callbacks)

from keras.models import load_model

model = load_model('twss_model.h5')

sentence = "It's too big for me to handle"
sequence = tokenizer.texts_to_sequences([sentence])
data = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH)
result = model.predict(data)

if np.argmax(result) == 1:
    print("That's what she said")
else:
    print("Not a TWSS joke.")